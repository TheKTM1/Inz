{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "#plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'input.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#data = open('shakespear.txt', 'r').read()\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input.txt'"
     ]
    }
   ],
   "source": [
    "data = open('input.txt', 'r').read()\n",
    "#data = open('shakespear.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data and calculate indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 99993 characters, 62 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "H_size = 100 # Size of the hidden layer\n",
    "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
    "learning_rate = 1e-1 # Learning rate\n",
    "weight_sd = 0.1 # Standard deviation of weights for initialization\n",
    "z_size = H_size + X_size # Size of concatenate(H, X) vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions and Derivatives\n",
    "\n",
    "#### Sigmoid\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\\\\\n",
    "\\frac{d\\sigma(x)}{dx} &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align}\n",
    "\n",
    "#### Tanh\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d\\text{tanh}(x)}{dx} &= 1 - \\text{tanh}^2(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Param:\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.v = value #parameter value\n",
    "        self.d = np.zeros_like(value) #derivative\n",
    "        self.m = np.zeros_like(value) #momentum for AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use random weights with normal distribution (`0`, `weight_sd`) for $tanh$ activation function and (`0.5`, `weight_sd`) for $sigmoid$ activation function.\n",
    "\n",
    "Biases are initialized to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.W_f = Param('W_f', \n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_f = Param('b_f',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_i = Param('W_i',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_i = Param('b_i',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_C = Param('W_C',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd)\n",
    "        self.b_C = Param('b_C',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_o = Param('W_o',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_o = Param('b_o',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        #For final layer to predict the next character\n",
    "        self.W_v = Param('W_v',\n",
    "                         np.random.randn(X_size, H_size) * weight_sd)\n",
    "        self.b_v = Param('b_v',\n",
    "                         np.zeros((X_size, 1)))\n",
    "        \n",
    "    def all(self):\n",
    "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
    "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
    "        \n",
    "parameters = Parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "![LSTM](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
    "\n",
    "*Operation $z$ is the concatenation of $x$ and $h_{t-1}$*\n",
    "\n",
    "#### Concatenation of $h_{t-1}$ and $x_t$\n",
    "\\begin{align}\n",
    "z & = [h_{t-1}, x_t] \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### LSTM functions\n",
    "\\begin{align}\n",
    "f_t & = \\sigma(W_f \\cdot z + b_f) \\\\\n",
    "i_t & = \\sigma(W_i \\cdot z + b_i) \\\\\n",
    "\\bar{C}_t & = tanh(W_C \\cdot z + b_C) \\\\\n",
    "C_t & = f_t * C_{t-1} + i_t * \\bar{C}_t \\\\\n",
    "o_t & = \\sigma(W_o \\cdot z + b_t) \\\\\n",
    "h_t &= o_t * tanh(C_t) \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Logits\n",
    "\\begin{align}\n",
    "v_t &= W_v \\cdot h_t + b_v \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Softmax\n",
    "\\begin{align}\n",
    "\\hat{y_t} &= \\text{softmax}(v_t)\n",
    "\\end{align}\n",
    "\n",
    "$\\hat{y_t}$ is `y` in code and $y_t$ is `targets`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forward(x, h_prev, C_prev, p = parameters):\n",
    "    assert x.shape == (X_size, 1)\n",
    "    assert h_prev.shape == (H_size, 1)\n",
    "    assert C_prev.shape == (H_size, 1)\n",
    "    \n",
    "    z = np.row_stack((h_prev, x))\n",
    "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
    "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
    "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
    "\n",
    "    C = f * C_prev + i * C_bar\n",
    "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
    "    h = o * tanh(C)\n",
    "\n",
    "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
    "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
    "\n",
    "    return z, f, i, C_bar, C, o, h, v, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "#### Loss\n",
    "\n",
    "\\begin{align}\n",
    "L_k &= -\\sum_{t=k}^T\\sum_j y_{t,j} log \\hat{y_{t,j}} \\\\\n",
    "L &= L_1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Gradients\n",
    "\n",
    "\\begin{align}\n",
    "dv_t &= \\hat{y_t} - y_t \\\\\n",
    "dh_t &= dh'_t + W_y^T \\cdot dv_t \\\\\n",
    "do_t &= dh_t * \\text{tanh}(C_t) \\\\\n",
    "dC_t &= dC'_t + dh_t * o_t * (1 - \\text{tanh}^2(C_t))\\\\\n",
    "d\\bar{C}_t &= dC_t * i_t \\\\\n",
    "di_t &= dC_t * \\bar{C}_t \\\\\n",
    "df_t &= dC_t * C_{t-1} \\\\\n",
    "\\\\\n",
    "df'_t &= f_t * (1 - f_t) * df_t \\\\\n",
    "di'_t &= i_t * (1 - i_t) * di_t \\\\\n",
    "d\\bar{C}'_{t-1} &= (1 - \\bar{C}_t^2) * d\\bar{C}_t \\\\\n",
    "do'_t &= o_t * (1 - o_t) * do_t \\\\\n",
    "dz_t &= W_f^T \\cdot df'_t \\\\\n",
    "     &+ W_i^T \\cdot di_t \\\\\n",
    "     &+ W_C^T \\cdot d\\bar{C}_t \\\\\n",
    "     &+ W_o^T \\cdot do_t \\\\\n",
    "\\\\\n",
    "[dh'_{t-1}, dx_t] &= dz_t \\\\\n",
    "dC'_t &= f_t * dC_t\n",
    "\\end{align}\n",
    "\n",
    "* $dC'_t = \\frac{\\partial L_{t+1}}{\\partial C_t}$ and $dh'_t = \\frac{\\partial L_{t+1}}{\\partial h_t}$\n",
    "* $dC_t = \\frac{\\partial L}{\\partial C_t} = \\frac{\\partial L_t}{\\partial C_t}$ and $dh_t = \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L_{t}}{\\partial h_t}$\n",
    "* All other derivatives are of $L$\n",
    "* `target` is target character index $y_t$\n",
    "* `dh_next` is $dh'_{t}$ (size H x 1)\n",
    "* `dC_next` is $dC'_{t}$ (size H x 1)\n",
    "* `C_prev` is $C_{t-1}$ (size H x 1)\n",
    "* $df'_t$, $di'_t$, $d\\bar{C}'_t$, and $do'_t$ are *also* assigned to `df`, `di`, `dC_bar`, and `do` in the **code**.\n",
    "* *Returns* $dh_t$ and $dC_t$\n",
    "\n",
    "#### Model parameter gradients\n",
    "\n",
    "\\begin{align}\n",
    "dW_v &= dv_t \\cdot h_t^T \\\\\n",
    "db_v &= dv_t \\\\\n",
    "\\\\\n",
    "dW_f &= df'_t \\cdot z^T \\\\\n",
    "db_f &= df'_t \\\\\n",
    "\\\\\n",
    "dW_i &= di'_t \\cdot z^T \\\\\n",
    "db_i &= di'_t \\\\\n",
    "\\\\\n",
    "dW_C &= d\\bar{C}'_t \\cdot z^T \\\\\n",
    "db_C &= d\\bar{C}'_t \\\\\n",
    "\\\\\n",
    "dW_o &= do'_t \\cdot z^T \\\\\n",
    "db_o &= do'_t \\\\\n",
    "\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(target, dh_next, dC_next, C_prev,\n",
    "             z, f, i, C_bar, C, o, h, v, y,\n",
    "             p = parameters):\n",
    "    \n",
    "    assert z.shape == (X_size + H_size, 1)\n",
    "    assert v.shape == (X_size, 1)\n",
    "    assert y.shape == (X_size, 1)\n",
    "    \n",
    "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "        assert param.shape == (H_size, 1)\n",
    "        \n",
    "    dv = np.copy(y)\n",
    "    dv[target] -= 1\n",
    "\n",
    "    p.W_v.d += np.dot(dv, h.T)\n",
    "    p.b_v.d += dv\n",
    "\n",
    "    dh = np.dot(p.W_v.v.T, dv)        \n",
    "    dh += dh_next\n",
    "    do = dh * tanh(C)\n",
    "    do = dsigmoid(o) * do\n",
    "    p.W_o.d += np.dot(do, z.T)\n",
    "    p.b_o.d += do\n",
    "\n",
    "    dC = np.copy(dC_next)\n",
    "    dC += dh * o * dtanh(tanh(C))\n",
    "    dC_bar = dC * i\n",
    "    dC_bar = dtanh(C_bar) * dC_bar\n",
    "    p.W_C.d += np.dot(dC_bar, z.T)\n",
    "    p.b_C.d += dC_bar\n",
    "\n",
    "    di = dC * C_bar\n",
    "    di = dsigmoid(i) * di\n",
    "    p.W_i.d += np.dot(di, z.T)\n",
    "    p.b_i.d += di\n",
    "\n",
    "    df = dC * C_prev\n",
    "    df = dsigmoid(f) * df\n",
    "    p.W_f.d += np.dot(df, z.T)\n",
    "    p.b_f.d += df\n",
    "\n",
    "    dz = (np.dot(p.W_f.v.T, df)\n",
    "         + np.dot(p.W_i.v.T, di)\n",
    "         + np.dot(p.W_C.v.T, dC_bar)\n",
    "         + np.dot(p.W_o.v.T, do))\n",
    "    dh_prev = dz[:H_size, :]\n",
    "    dC_prev = f * dC\n",
    "    \n",
    "    return dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear gradients before each backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.d.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip gradients to mitigate exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        np.clip(p.d, -1, 1, out=p.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
    "\n",
    "* `input`, `target` are list of integers, with character indexes.\n",
    "* `h_prev` is the array of initial `h` at $h_{-1}$ (size H x 1)\n",
    "* `C_prev` is the array of initial `C` at $C_{-1}$ (size H x 1)\n",
    "* *Returns* loss, final $h_T$ and $C_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forward_backward(inputs, targets, h_prev, C_prev):\n",
    "    global paramters\n",
    "    \n",
    "    # To store the values for each time step\n",
    "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
    "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
    "    v_s, y_s =  {}, {}\n",
    "    \n",
    "    # Values at t - 1\n",
    "    h_s[-1] = np.copy(h_prev)\n",
    "    C_s[-1] = np.copy(C_prev)\n",
    "    \n",
    "    loss = 0\n",
    "    # Loop through time steps\n",
    "    assert len(inputs) == T_steps\n",
    "    for t in range(len(inputs)):\n",
    "        x_s[t] = np.zeros((X_size, 1))\n",
    "        x_s[t][inputs[t]] = 1 # Input character\n",
    "        \n",
    "        (z_s[t], f_s[t], i_s[t],\n",
    "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
    "        v_s[t], y_s[t]) = \\\n",
    "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
    "            \n",
    "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
    "        \n",
    "    clear_gradients()\n",
    "\n",
    "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
    "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backward pass\n",
    "        dh_next, dC_next = \\\n",
    "            backward(target = targets[t], dh_next = dh_next,\n",
    "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
    "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
    "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
    "                     y = y_s[t])\n",
    "\n",
    "    clip_gradients()\n",
    "        \n",
    "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
    "    x = np.zeros((X_size, 1))\n",
    "    x[first_char_idx] = 1\n",
    "\n",
    "    h = h_prev\n",
    "    C = C_prev\n",
    "\n",
    "    indexes = []\n",
    "    \n",
    "    for t in range(sentence_length):\n",
    "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
    "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
    "        x = np.zeros((X_size, 1))\n",
    "        x[idx] = 1\n",
    "        indexes.append(idx)\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Adagrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the graph and display a sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_status(inputs, h_prev, C_prev):\n",
    "    #initialized later\n",
    "    global plot_iter, plot_loss\n",
    "    global smooth_loss\n",
    "    \n",
    "    # Get predictions for 200 letters with current model\n",
    "\n",
    "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
    "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
    "\n",
    "    # Clear and plot\n",
    "    plt.plot(plot_iter, plot_loss)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()\n",
    "\n",
    "    #Print prediction and loss\n",
    "    print(\"----\\n %s \\n----\" % (txt, ))\n",
    "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update parameters\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
    "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paramters(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.m += p.d * p.d # Calculate sum of gradients\n",
    "        #print(learning_rate * dparam)\n",
    "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delay the keyboard interrupt to prevent the training \n",
    "from stopping in the middle of an iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "class DelayedKeyboardInterrupt(object):\n",
    "    def __enter__(self):\n",
    "        self.signal_received = False\n",
    "        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n",
    "\n",
    "    def handler(self, sig, frame):\n",
    "        self.signal_received = (sig, frame)\n",
    "        print('SIGINT received. Delaying KeyboardInterrupt.')\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.signal(signal.SIGINT, self.old_handler)\n",
    "        if self.signal_received:\n",
    "            self.old_handler(*self.signal_received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exponential average of loss\n",
    "# Initialize to a error of a random model\n",
    "smooth_loss = -np.log(1.0 / X_size) * T_steps\n",
    "\n",
    "iteration, pointer = 0, 0\n",
    "\n",
    "# For the graph\n",
    "plot_iter = np.zeros((0))\n",
    "plot_loss = np.zeros((0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "  Led hive pritice of cremm's tolborrit deeser straish'd: with many, plains love that the so.\n",
      "\n",
      "Hecus I man .\n",
      "\n",
      "Were Sthy fly love jonturs and unher, ha?\n",
      "\n",
      "EPEET:\n",
      "And, no.\n",
      "\n",
      "EDCARS:\n",
      "Is eveng\n",
      "A dome thore w \n",
      "----\n",
      "iter 29500, loss 41.127999\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        with DelayedKeyboardInterrupt():\n",
    "            # Reset\n",
    "            if pointer + T_steps >= len(data) or iteration == 0:\n",
    "                g_h_prev = np.zeros((H_size, 1))\n",
    "                g_C_prev = np.zeros((H_size, 1))\n",
    "                pointer = 0\n",
    "\n",
    "\n",
    "            inputs = ([char_to_idx[ch] \n",
    "                       for ch in data[pointer: pointer + T_steps]])\n",
    "            targets = ([char_to_idx[ch] \n",
    "                        for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
    "\n",
    "            loss, g_h_prev, g_C_prev = \\\n",
    "                forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "            # Print every hundred steps\n",
    "            if iteration % 100 == 0:\n",
    "                update_status(inputs, g_h_prev, g_C_prev)\n",
    "\n",
    "            update_paramters()\n",
    "\n",
    "            plot_iter = np.append(plot_iter, [iteration])\n",
    "            plot_loss = np.append(plot_loss, [loss])\n",
    "\n",
    "            pointer += T_steps\n",
    "            iteration += 1\n",
    "    except KeyboardInterrupt:\n",
    "        update_status(inputs, g_h_prev, g_C_prev)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Check\n",
    "\n",
    "Approximate the numerical gradients by changing parameters and running the model. Check if the approximated gradients are equal to the computed analytical gradients (by backpropagation).\n",
    "\n",
    "Try this on `num_checks` individual paramters picked randomly for each weight matrix and bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate numerical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_numerical_gradient(param, idx, delta, inputs, target, h_prev, C_prev):\n",
    "    old_val = param.v.flat[idx]\n",
    "    \n",
    "    # evaluate loss at [x + delta] and [x - delta]\n",
    "    param.v.flat[idx] = old_val + delta\n",
    "    loss_plus_delta, _, _ = forward_backward(inputs, targets,\n",
    "                                             h_prev, C_prev)\n",
    "    param.v.flat[idx] = old_val - delta\n",
    "    loss_mins_delta, _, _ = forward_backward(inputs, targets, \n",
    "                                             h_prev, C_prev)\n",
    "    \n",
    "    param.v.flat[idx] = old_val #reset\n",
    "\n",
    "    grad_numerical = (loss_plus_delta - loss_mins_delta) / (2 * delta)\n",
    "    # Clip numerical error because analytical gradient is clipped\n",
    "    [grad_numerical] = np.clip([grad_numerical], -1, 1) \n",
    "    \n",
    "    return grad_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient of each paramter matrix/vector at `num_checks` individual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_check(num_checks, delta, inputs, target, h_prev, C_prev):\n",
    "    global parameters\n",
    "    \n",
    "    # To calculate computed gradients\n",
    "    _, _, _ =  forward_backward(inputs, targets, h_prev, C_prev)\n",
    "    \n",
    "    \n",
    "    for param in parameters.all():\n",
    "        #Make a copy because this will get modified\n",
    "        d_copy = np.copy(param.d)\n",
    "\n",
    "        # Test num_checks times\n",
    "        for i in range(num_checks):\n",
    "            # Pick a random index\n",
    "            rnd_idx = int(uniform(0, param.v.size))\n",
    "            \n",
    "            grad_numerical = calc_numerical_gradient(param,\n",
    "                                                     rnd_idx,\n",
    "                                                     delta,\n",
    "                                                     inputs,\n",
    "                                                     target,\n",
    "                                                     h_prev, C_prev)\n",
    "            grad_analytical = d_copy.flat[rnd_idx]\n",
    "\n",
    "            err_sum = abs(grad_numerical + grad_analytical) + 1e-09\n",
    "            rel_error = abs(grad_analytical - grad_numerical) / err_sum\n",
    "            \n",
    "            # If relative error is greater than 1e-06\n",
    "            if rel_error > 1e-06:\n",
    "                print('%s (%e, %e) => %e'\n",
    "                      % (param.name, grad_numerical, grad_analytical, rel_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-55851ca8fe54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgradient_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_h_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_C_prev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "gradient_check(10, 1e-5, inputs, targets, g_h_prev, g_C_prev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
